{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv('Titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of the column PassengerId: 66231.0\n",
      "Variance of the column Survived: 0.2367722165474984\n",
      "Variance of the column Pclass: 0.6990151199889065\n",
      "Variance of the column Age: 169.05239993721085\n",
      "Variance of the column SibSp: 1.2160430774662894\n",
      "Variance of the column Parch: 0.6497282437357467\n",
      "Variance of the column Fare: 2469.436845743117\n"
     ]
    }
   ],
   "source": [
    "def calculate_variance(data):\n",
    "    n = len(data)\n",
    "    if n <= 1:\n",
    "        return 0.0\n",
    "    mean = sum(data) / n\n",
    "    variance = sum((x - mean) ** 2 for x in data) / (n - 1)\n",
    "    return variance\n",
    "\n",
    "for i in data.columns:\n",
    "    if data[i].dtype == 'int64' or data[i].dtype == 'float64':\n",
    "        print(f\"Variance of the column {i}:\",calculate_variance(data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "data = data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)  # drop irrelevant columns\n",
    "data['Age'] = data['Age'].fillna(data['Age'].mean())  # fill missing values with mean\n",
    "data = pd.get_dummies(data)  # one-hot encode categorical variables\n",
    "\n",
    "# split data into training and test sets\n",
    "X = data.drop('Survived', axis=1)\n",
    "y = data['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7821229050279329\n"
     ]
    }
   ],
   "source": [
    "# train decision tree\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8324022346368715\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest model object\n",
    "'''\n",
    "Some Important Parameters:-\n",
    "1. n_estimators:- It defines the number of decision trees to be created in a random forest.\n",
    "2. criterion:- \"Gini\" or \"Entropy.\"\n",
    "3. min_samples_split:- Used to define the minimum number of samples required in a leaf \n",
    "node before a split is attempted\n",
    "4. max_features: -It defines the maximum number of features allowed for the split in each \n",
    "decision tree.\n",
    "5. n_jobs:- The number of jobs to run in parallel for both fit and predict. Always keep (-1) to \n",
    "use all the cores for parallel processing.\n",
    "'''\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, min_samples_split=5, random_state=42)\n",
    "\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Use the model to make predictions on the testing data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7821229050279329\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Kernel: The kernel function is used to transform the input data into a higher-dimensional space, where it may be easier to separate the classes. \n",
    "The most common types of kernel functions are linear, polynomial, and radial basis function (RBF).\n",
    "\n",
    "C: C is the regularization parameter, which controls the trade-off between maximizing the margin and minimizing the classification error. \n",
    "A large value of C will result in a smaller margin and more classification errors, while a small value of C will result in a larger margin but may lead to more misclassifications.\n",
    "\n",
    "Gamma: Gamma is a parameter of the RBF kernel function that controls the width of the kernel. \n",
    "A smaller value of gamma will result in a wider kernel, which may result in more points being considered support vectors. A larger value of gamma will result in a narrower kernel, which may result in overfitting.\n",
    "\n",
    "Degree: Degree is a parameter of the polynomial kernel function that controls the degree of the polynomial. \n",
    "A higher degree polynomial will result in a more complex decision boundary, which may lead to overfitting.\n",
    "\n",
    "Class Weight: Class weight is a parameter that is used to balance the weight of the different classes. \n",
    "This is useful when the classes are imbalanced, and the SVM model may be biased towards the majority class.\n",
    "\n",
    "Probability: The probability parameter allows the SVM model to output the probability estimates of the predicted class.\n",
    "'''\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1.0, gamma='auto',degree=3,probability=True)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best kernel function:  linear\n",
      "Best score:  0.7878853540825371\n"
     ]
    }
   ],
   "source": [
    "# Create a support vector classifier object with grid search\n",
    "clf = svm.SVC()\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    'degree': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best kernel function\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best kernel function and its score\n",
    "print(\"Best kernel function: \", grid_search.best_params_['kernel'])\n",
    "print(\"Best score: \", grid_search.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.776536312849162\n"
     ]
    }
   ],
   "source": [
    "clf = naive_bayes.GaussianNB()\n",
    "# clf = naive_bayes.MultinomialNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "y_prob = clf.predict_proba(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
